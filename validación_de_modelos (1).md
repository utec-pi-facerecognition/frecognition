# -*- coding: utf-8 -*-
"""Validación de modelos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18rC9aV6pLeBz1JHMQKFZu3N0SNzu7iRa

### Modelos : 

###![texto alternativo](https://blog.bismart.com/hs-fs/hubfs/Machinne%20Learning%20Types%20Bismart.png?width=900&name=Machinne%20Learning%20Types%20Bismart.png)

Siempre se quiere evaluar la efectividad del modelo En la mayoría de las aplicaciones (aunque no todas), la medida relevante de la calidad del modelo es la precisión predictiva. En otras palabras, qué tan cerca están las predicciones del modelo de lo que realmente sucede.

MAE: Se calcula el error para cada predicción individual, se saca el valor absoluto y se promedian todas las predicciones. En otras palabras, nuestro predicciones están incorrectas por más o menos X
"""

from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state=1)
predicted  = model.predict(X)
mean_absolute_error(y, predicted)

"""Underfitting : Cuando el modelo no tiene buenos resultados porque no está suficientemente entrenado

Overfitting : Cuando el modelo no tiene buenos resultados porque está muy acorde a la data de testeo

![texto alternativo](https://i0.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2017/12/overfitting-underfitting-machine-learning.png?w=800&ssl=1
)

### k-Fold Cross-Validation : En lugar de dividir solo una vez el dataset , lo dividimos varias veces. Se separa el modelo en K bloques y se entrena en K-1 , reservando el último para testeo ![texto alternativo](https://i.imgur.com/9k60cVA.png)
"""

# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(x_test, y_test, batch_size=128)
print('test loss, test acc:', results)

# Generate predictions (probabilities -- the output of the last layer)
# on new data using `predict`
print('\n# Generate predictions for 3 samples')
predictions = model.predict(x_test[:3])
print('predictions shape:', predictions.shape)

MeanSquaredError()
KLDivergence()
CosineSimilarity()


tf.estimator.Estimator(
    model_fn, model_dir=None, config=None, params=None, warm_start_from=None
)

from sklearn.model_selection import cross_val_score

# Multiply by -1 since sklearn calculates *negative* MAE
scores = -1 * cross_val_score(my_pipeline, X, y,
                              cv=5,
                              scoring='neg_mean_absolute_error')



"""https://www.kaggle.com/
https://towardsdatascience.com/
https://www.tensorflow.org/ https://www.aprendemachinelearning.com/
"""